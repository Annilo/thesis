{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af155f7-39ef-491b-acb7-c2a28c9ee31c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pip install pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "610ca01f-93c9-4209-941d-80c6f621b2b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, struct, explode, collect_list,lit,array,split, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType,FloatType,DoubleType,IntegerType,TimestampType\n",
    "from time import sleep\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import DataFrame,SparkSession\n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f43dea1-c1c1-4ce1-b985-f34c44081a03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "041b5ed9-7022-4c97-952c-f145cf5ba19b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class AlphabetService:\n",
    "    def __init__(self):\n",
    "        self.activity_to_alphabet = {}\n",
    "        self.alphabet_to_activity = {}\n",
    "        self.char_counter = 64  \n",
    "\n",
    "    def alphabetize(self, label):\n",
    "        if label not in self.activity_to_alphabet:\n",
    "            self.char_counter += 1\n",
    "            self.activity_to_alphabet[label] = chr(self.char_counter)\n",
    "            self.alphabet_to_activity[chr(self.char_counter)] = label\n",
    "            #print(\"label\",label,\"char\",chr(self.char_counter))\n",
    "        return self.activity_to_alphabet[label]\n",
    "\n",
    "    def clear(self):\n",
    "        self.activity_to_alphabet.clear()\n",
    "        self.alphabet_to_activity.clear()\n",
    "        self.char_counter = 64\n",
    "\n",
    "\n",
    "def process_trace(trace, nodes,depth, current_node='root'):\n",
    "    last_node = current_node #root\n",
    "    previous_n_nodes = ['root']\n",
    "\n",
    "    nodes['root'] = {'label': 'root', 'parent': None, 'level':0, 'direct_children': set(),'direct_children_labels':set(),'nth_children':set()}\n",
    "    for event in trace:\n",
    "        event_name = event['alphabetized_label']\n",
    "        node_id = f\"{last_node}-{event_name}\"\n",
    "        if node_id not in nodes:\n",
    "            nodes[node_id] = {'label': event_name, 'parent': last_node if last_node != 'root' else 'root', 'level':int((len(node_id)-4)/2), 'direct_children': set(),'direct_children_labels':set(),'nth_children':set()}\n",
    "        \n",
    "        #update the children of the parent node\n",
    "        if len(previous_n_nodes)>depth:\n",
    "            previous_n_nodes = previous_n_nodes[1:]\n",
    "        for node in previous_n_nodes:\n",
    "            start = len(node)\n",
    "            end = len(node_id)-1\n",
    "            events_between = tuple(node_id[start+1:end-1].split(\"-\"))\n",
    "            sub_array = (node_id,event_name,int((len(node_id)-4)/2),events_between)\n",
    "            nodes[node]['nth_children'].add(sub_array)\n",
    "        if last_node != 'root':\n",
    "            nodes[last_node]['direct_children'].add(node_id)\n",
    "            nodes[last_node]['direct_children_labels'].add(event_name)\n",
    "        previous_n_nodes.append(node_id)\n",
    "        last_node = node_id\n",
    "\n",
    "    return nodes\n",
    "\n",
    "def convert_sets_to_lists(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_sets_to_lists(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "event_log = pm4py.read_xes(\"/tmp/BPI_2012_1k_sample.xes\")\n",
    "dataframe = log_converter.apply(event_log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "labels_trace = dataframe[[\"concept:name\", \"case:concept:name\"]]\n",
    "grouped_traces = labels_trace.groupby(\"case:concept:name\", sort=False)\n",
    "\n",
    "alphabet_service = AlphabetService()\n",
    "nodes = {}\n",
    "for trace, group in grouped_traces:\n",
    "    group['alphabetized_label'] = group[\"concept:name\"].apply(alphabet_service.alphabetize)\n",
    "    nodes = process_trace(group.to_dict('records'), nodes, 3)\n",
    "\n",
    "labels = alphabet_service.activity_to_alphabet\n",
    "schema = StructType([\n",
    "    StructField(\"event\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), True)\n",
    "])\n",
    "df_labels = spark.createDataFrame(list(labels.items()), schema=schema)\n",
    "\n",
    "data = [{\"node_id\": node_id, **convert_sets_to_lists(node_data)} for node_id, node_data in nodes.items()]\n",
    "#df_nodes = spark.createDataFrame(data)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"node_id\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"level\", IntegerType(), True),\n",
    "    StructField(\"direct_children\", ArrayType(StringType()), True),\n",
    "    StructField(\"direct_children_labels\", ArrayType(StringType()), True),\n",
    "    StructField(\"nth_children\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"node_id\", StringType(), True),\n",
    "            StructField(\"label\", StringType(), True),\n",
    "            StructField(\"level\", IntegerType(), True),\n",
    "            StructField(\"events_between\", ArrayType(StringType()), True)\n",
    "        ])\n",
    "    ), True),\n",
    "    StructField(\"parent\", StringType(), True)\n",
    "])\n",
    "df_nodes = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9608835-90ff-4c9c-9c4c-f83f8de5aced",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_nodes.createOrReplaceTempView(\"iws_model\")\n",
    "df_labels.createOrReplaceTempView(\"iws_labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "200fc61e-09a4-4d26-bc25-1a674e79fd87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE iws_event (event STRING, time_stamp TIMESTAMP, trace_id STRING);\n",
    "\n",
    "CREATE OR REPLACE TABLE iws_state\n",
    "(trace_id STRING, ts TIMESTAMP, current_node STRING,current_id STRING,cost_of_alignment INTEGER,previous_events STRING, trace STRING, execution_sequence STRING,event_level INTEGER,current_event_level INTEGER,current_node_level INTEGER);\n",
    "--event level to filter out the latest alignments later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40dc62d8-f5db-4127-a65c-22f4cf645924",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "event_df = spark.readStream.table(\"iws_event\").withWatermark(\"time_stamp\", \"1 minute\")\n",
    "event_df.createOrReplaceTempView(\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31504bad-5186-4d75-9b95-906410f15e59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "def calculateAlignmentCost(modelEvents: String, eventArray: Array[String]): Array[(String, String, Int, Int)] = {\n",
    "  val newEvents = modelEvents.replace(\"-\", \"\").split(\"\")\n",
    "  val n = eventArray.length\n",
    "  val m = newEvents.length\n",
    "  val dp = Array.tabulate(n + 1, m + 1)((i, j) => if (i == 0) j else if (j == 0) i else 0)\n",
    "\n",
    "  // calculate the matrix\n",
    "  for (i <- 1 to n) {\n",
    "    for (j <- 1 to m) {\n",
    "      if (eventArray(i - 1) == newEvents(j - 1)) {\n",
    "        dp(i)(j) = dp(i - 1)(j - 1)\n",
    "      } else {\n",
    "        dp(i)(j) = math.min(dp(i - 1)(j) + 1, dp(i)(j - 1) + 1)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // track back\n",
    "  val alignment = ArrayBuffer[(String, String, Int, Int)]()\n",
    "  var i = n\n",
    "  var j = m\n",
    "  var cost = dp(n)(m)\n",
    "\n",
    "  while (i > 0 && j > 0) {\n",
    "    if (eventArray(i - 1) == newEvents(j - 1)) {\n",
    "      alignment.prepend((eventArray(i - 1), \"sync\", 0, i))\n",
    "      i -= 1\n",
    "      j -= 1\n",
    "    } else if (dp(i)(j) == dp(i - 1)(j) + 1) {\n",
    "      alignment.prepend((eventArray(i - 1), \"log\", 1, i))\n",
    "      i -= 1\n",
    "    } else {\n",
    "      alignment.prepend((newEvents(j - 1), \"model\", 1, j))\n",
    "      j -= 1\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // handle any elements left over \n",
    "  while (i > 0) {\n",
    "    alignment.prepend((eventArray(i - 1), \"log\", 1, i))\n",
    "    i -= 1\n",
    "  }\n",
    "  while (j > 0) {\n",
    "    alignment.prepend((newEvents(j - 1), \"model\", 1, j))\n",
    "    j -= 1\n",
    "  }\n",
    "\n",
    "  alignment.toArray\n",
    "}\n",
    "\n",
    "// Registering the UDF\n",
    "spark.udf.register(\"calculateAlignmentCost\", (modelEvents: String, eventArray: Array[String]) => calculateAlignmentCost(modelEvents, eventArray))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abab9ab3-5c2d-484d-9319-3adee8265b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Solution 1: Using complete output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e17475cd-7f38-42dd-ae7c-cec5c7865466",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE state_test_comp\n",
    "(trace_id STRING, event_array ARRAY<STRUCT<time_stamp TIMESTAMP,label STRING>>,len INTEGER,previous_id STRING,cost_of_alignment INTEGER,previous_events STRING,previous_alignment ARRAY<STRUCT<event String,move_type String>>,event_level INTEGER,current_event_level INTEGER,current_node_level INTEGER);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5631bee-7c64-4249-9c2a-2c6ef3358263",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW POSSIBLE_ALIGNMENTS AS (\n",
    "SELECT trace_id, node_id as current_id ,cost_of_alignment + ARRAY_MAX(calc_alignment.cost) AS cost_of_alignment,concat(previous_events,concat_ws(\"\",current_events)) as previous_events,CONCAT(\n",
    "                    previous_alignment,\n",
    "                    TRANSFORM(\n",
    "                        calc_alignment,\n",
    "                            x -> named_struct('event', x.event, 'move_type', x.move_type)\n",
    "                        )\n",
    "                 ) AS alignment,current_event_level + size(current_events) as event_level,event_index,time_stamp,incoming_label,level as current_node_level,current_events FROM (\n",
    "SELECT *,calculate_alignment(previous_id,node_id,current_events) as calc_alignment \n",
    "FROM (SELECT DISTINCT *, idx AS event_index, \n",
    "                col.time_stamp as time_stamp,\n",
    "                col.label as incoming_label,\n",
    "                slice(event_array,current_event_level+1,len - current_event_level).label as current_events  \n",
    "                from state_test_comp\n",
    "                LATERAL VIEW posexplode(slice(event_array,current_event_level+1,len-current_event_level)) AS idx, col) f\n",
    "                JOIN iws_model m ON (m.node_id LIKE CONCAT(f.previous_id, '%') AND\n",
    "                m.level < f.current_node_level + event_index + 3 AND\n",
    "                m.label = f.incoming_label) OR\n",
    "                m.node_id = f.previous_id) );\n",
    "\n",
    "CREATE OR REPLACE temp VIEW stream_test_alignm AS SELECT DISTINCT \n",
    "                trace_id,\n",
    "                current_id,\n",
    "                previous_events,\n",
    "                alignment,\n",
    "                cost_of_alignment,\n",
    "                event_level,\n",
    "                max_event_level as current_event_level,\n",
    "                current_node_level\n",
    "FROM   (\n",
    "        SELECT *,\n",
    "        Max(event_level) OVER (partition BY trace_id) AS max_event_level,\n",
    "        Row_number() OVER (partition BY trace_id,current_id ORDER BY event_level DESC,cost_of_alignment ASC, Len(previous_events) DESC) rn\n",
    "        FROM POSSIBLE_ALIGNMENTS)\n",
    "WHERE rn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c6a7718-7148-4d7f-a2d2-2c49078a9013",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT\n",
    "            e.trace_id as trace_id,\n",
    "            e.event_array as event_array,\n",
    "            size(e.event_array) as len,\n",
    "            COALESCE(r.current_id, 'root') AS previous_id,\n",
    "            COALESCE(r.cost_of_alignment, 0) AS cost_of_alignment,\n",
    "            COALESCE(r.previous_events, '') AS previous_events,\n",
    "            COALESCE(r.alignment, ARRAY(struct(\"\" AS event, \"\" AS move_type))) AS previous_alignment,\n",
    "            COALESCE(r.event_level, 0) AS event_level,\n",
    "            COALESCE(r.current_event_level, 0) AS current_event_level,\n",
    "            COALESCE(r.current_node_level, 0) AS current_node_level\n",
    "            FROM (\n",
    "                SELECT trace_id,\n",
    "                        array_sort(collect_list(struct(time_stamp, label))) AS event_array\n",
    "                    FROM \n",
    "                        events e\n",
    "                    JOIN \n",
    "                        iws_labels l ON e.event = l.event\n",
    "                    GROUP BY trace_id) e\n",
    "            LEFT JOIN stream_test_alignm r ON e.trace_id = r.trace_id\n",
    "\"\"\").writeStream.format(\"delta\").outputMode(\"complete\").option(\"checkpointLocation\", \"/tmp/delta/state_append_30079/\").toTable(\"state_test_comp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19d355e0-0155-4c95-b030-0432ebd642c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "foreachBatch method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58ba0128-4e86-4dd7-932d-37d1aead9f95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE state_test_batch\n",
    "(trace_id STRING, ts TIMESTAMP,current_id STRING,previous_events STRING,event_level INTEGER,current_node_level INTEGER,label STRING,alignment ARRAY<STRUCT<event String,move_type String>>,cost_of_alignment INTEGER,event_array ARRAY<STRING>,event_index INTEGER,batch_id INTEGER);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec565ae-2601-4b7f-9466-147658e2447c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE temp VIEW stream_test_alignm_batch AS SELECT DISTINCT \n",
    "                trace_id,\n",
    "                current_id,\n",
    "                previous_events,\n",
    "                alignment,\n",
    "                cost_of_alignment,\n",
    "                event_level,\n",
    "                max_event_level as current_event_level,\n",
    "                current_node_level,\n",
    "                rn,\n",
    "                event_array,\n",
    "                event_index,\n",
    "                batch_id\n",
    "FROM   (\n",
    "        SELECT *,\n",
    "        Max(event_level) OVER (partition BY trace_id) AS max_event_level,\n",
    "        Row_number() OVER (partition BY trace_id,current_id ORDER BY event_level DESC,cost_of_alignment ASC, Len(previous_events) DESC) rn\n",
    "        FROM state_test_batch)\n",
    "WHERE rn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf11e04-7859-4bca-bb05-92a13e926ce6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from iws_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2821e2ee-59ac-4d57-b785-baa6fc92286e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_batch(df: DataFrame, batch_id: int):\n",
    "\n",
    "    if not df.isEmpty():\n",
    "        df.createOrReplaceTempView(\"streaming_data\")\n",
    "\n",
    "        result_df = df.sparkSession.sql(\"\"\"\n",
    "            WITH FIRST_BD AS (\n",
    "            SELECT *,substr(node_id FROM len(previous_id) + 1) model_sub, CASE WHEN len(substr(node_id FROM len(previous_id) + 1)) = 0 THEN 0 ELSE _event_index END as event_index  FROM (\n",
    "            SELECT\n",
    "                e.trace_id AS trace_id,\n",
    "                idx AS _event_index,\n",
    "                col.time_stamp AS time_stamp,\n",
    "                col.label AS incoming_label,\n",
    "                e.event_array.label AS event_array,\n",
    "                size(e.event_array) AS len,\n",
    "                COALESCE(r.current_id, 'root') AS previous_id,\n",
    "                COALESCE(r.cost_of_alignment, 0) AS cost_of_alignment,\n",
    "                COALESCE(r.previous_events, '') AS previous_events,\n",
    "                substr(concat_ws(\"\", e.event_array.label), greatest(idx-3,0), idx + 1) AS trace_suffix,\n",
    "                COALESCE(r.alignment, ARRAY(struct(\"\" AS event, \"\" AS move_type))) AS previous_alignment,\n",
    "                COALESCE(r.event_level, 0) AS event_level,\n",
    "                COALESCE(r.current_event_level, 0) AS current_event_level,\n",
    "                COALESCE(r.current_node_level, 0) AS current_node_level\n",
    "            FROM (\n",
    "                SELECT \n",
    "                    trace_id,\n",
    "                    array_sort(collect_list(struct(time_stamp, label))) AS event_array\n",
    "                FROM \n",
    "                    streaming_data e \n",
    "                JOIN \n",
    "                    iws_labels l ON e.event = l.event\n",
    "                GROUP BY trace_id\n",
    "            ) e\n",
    "            LEFT JOIN stream_test_alignm_batch r ON e.trace_id = r.trace_id \n",
    "            LATERAL VIEW posexplode(e.event_array) AS idx, col\n",
    "        ) f\n",
    "        JOIN iws_model m ON (m.node_id LIKE CONCAT(f.previous_id, '%') \n",
    "            AND m.level < f.current_node_level  + _event_index + len - len(trace_suffix) + 2\n",
    "            AND m.label = f.incoming_label) \n",
    "            OR (m.node_id = f.previous_id AND len = _event_index + 1)),\n",
    "MaxEventIndexPerTrace AS (\n",
    "    SELECT\n",
    "        trace_id as _trace_id,\n",
    "        MAX(event_index) AS max_event_index\n",
    "    FROM FIRST_BD\n",
    "    GROUP BY trace_id\n",
    "),\n",
    "BASE_DATA AS (SELECT * FROM FIRST_BD LEFT JOIN MaxEventIndexPerTrace on FIRST_BD.trace_id = MaxEventIndexPerTrace._trace_id WHERE event_index > greatest(max_event_index-4,0))\n",
    "\n",
    "SELECT \n",
    "      trace_id,\n",
    "                time_stamp as ts,\n",
    "                node_id as current_id,\n",
    "                concat(previous_events,concat_ws(\"\",event_array)) as previous_events,\n",
    "                current_event_level + len as event_level,\n",
    "                level as current_node_level,\n",
    "                label,\n",
    "                CONCAT(\n",
    "                    previous_alignment,\n",
    "                    TRANSFORM(\n",
    "                        calc_alignment,\n",
    "                            x -> named_struct('event', x._1, 'move_type', x._2)\n",
    "                        )\n",
    "                 ) AS alignment,\n",
    "                cost_of_alignment + ARRAY_MAX(calc_alignment._3) AS cost_of_alignment,\n",
    "                event_array,\n",
    "                event_index\n",
    "    FROM (\n",
    "        SELECT *,calculateAlignmentCost(substr(node_id FROM len(previous_id) + 1),event_array) as calc_alignment \n",
    "        FROM BASE_DATA\n",
    "            ) \n",
    "        \"\"\")\n",
    "        result_df = result_df.withColumn(\"batch_id\",F.lit(batch_id))\n",
    "        # Write the results of the SQL query to a Delta table\n",
    "        result_df.write.format(\"delta\").mode(\"append\").option(\"checkpointLocation\", \"/tmp/delta/state_append_30038/\").saveAsTable(\"state_test_batch\")\n",
    "\n",
    "# Set up the write stream using foreachBatch\n",
    "query = streaming_df_batch_4.writeStream.foreachBatch(process_batch).start()\n",
    "#query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e1a70a-68c0-4919-b85f-d6df8f4b385b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Solution trying to optimise the amount of UDF calculations to be done, however cross-join is probaly too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "145e376c-da73-4c44-9296-c537fc448117",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_batch(df, batch_id):\n",
    "\n",
    "    if not df.isEmpty():\n",
    "        df.createOrReplaceTempView(\"streaming_data\")\n",
    "\n",
    "        result_df = df.sparkSession.sql(\"\"\"\n",
    "            WITH FIRST_BD AS (\n",
    "            SELECT *,substr(node_id FROM len(previous_id) + 1) model_sub, CASE WHEN len(substr(node_id FROM len(previous_id) + 1)) = 0 THEN 0 ELSE _event_index END as event_index  FROM (\n",
    "            SELECT\n",
    "                e.trace_id AS trace_id,\n",
    "                idx AS _event_index,\n",
    "                col.time_stamp AS time_stamp,\n",
    "                col.label AS incoming_label,\n",
    "                e.event_array.label AS event_array,\n",
    "                size(e.event_array) AS len,\n",
    "                COALESCE(r.current_id, 'root') AS previous_id,\n",
    "                COALESCE(r.cost_of_alignment, 0) AS cost_of_alignment,\n",
    "                COALESCE(r.previous_events, '') AS previous_events,\n",
    "                substr(concat_ws(\"\", e.event_array.label), greatest(idx-3,0), idx + 1) AS trace_suffix,\n",
    "                COALESCE(r.alignment, ARRAY(struct(\"\" AS event, \"\" AS move_type))) AS previous_alignment,\n",
    "                COALESCE(r.event_level, 0) AS event_level,\n",
    "                COALESCE(r.current_event_level, 0) AS current_event_level,\n",
    "                COALESCE(r.current_node_level, 0) AS current_node_level\n",
    "            FROM (\n",
    "                SELECT \n",
    "                    trace_id,\n",
    "                    array_sort(collect_list(struct(time_stamp, label))) AS event_array\n",
    "                FROM \n",
    "                    streaming_data e \n",
    "                JOIN \n",
    "                    iws_labels l ON e.event = l.event\n",
    "                GROUP BY trace_id\n",
    "            ) e\n",
    "            LEFT JOIN stream_test_alignm_batch r ON e.trace_id = r.trace_id \n",
    "            LATERAL VIEW posexplode(e.event_array) AS idx, col\n",
    "        ) f\n",
    "        JOIN iws_model m ON (m.node_id LIKE CONCAT(f.previous_id, '%') \n",
    "            AND m.level < f.current_node_level  + _event_index + 2\n",
    "            AND m.label = f.incoming_label) \n",
    "            OR (m.node_id = f.previous_id AND len = _event_index + 1)\n",
    "    ),\n",
    "MaxEventIndexPerTrace AS (\n",
    "    SELECT\n",
    "        trace_id as _trace_id,\n",
    "        MAX(event_index) AS max_event_index\n",
    "    FROM FIRST_BD\n",
    "    GROUP BY trace_id\n",
    "),\n",
    "BASE_DATA AS (SELECT * FROM FIRST_BD LEFT JOIN MaxEventIndexPerTrace on FIRST_BD.trace_id = MaxEventIndexPerTrace._trace_id WHERE event_index > greatest(max_event_index-4,0)),\n",
    "\n",
    "    SUFFIXES AS (\n",
    "        SELECT  * FROM (\n",
    "            SELECT  \n",
    "                a.previous_id, \n",
    "                a.trace_id, \n",
    "                a.node_id AS candidate,\n",
    "                MAX(CASE WHEN b.model_sub LIKE CONCAT(a.model_sub, '-%') THEN 1 ELSE 0 END) AS is_covered\n",
    "            FROM BASE_DATA a\n",
    "            JOIN BASE_DATA b ON a.model_sub <> b.model_sub \n",
    "                AND a.trace_id = b.trace_id \n",
    "                AND a.previous_id = b.previous_id\n",
    "            GROUP BY a.trace_id, a.previous_id,a.node_id\n",
    "        ) WHERE IS_COVERED = 0\n",
    "    ),\n",
    "    INTERMEDIATE AS (\n",
    "        SELECT \n",
    "            trace_id, \n",
    "            node_id AS current_id,\n",
    "            TRANSFORM(\n",
    "                calc_alignment,\n",
    "                x -> named_struct('event', x._1, 'move_type', x._2,\"_cost\",x._3, \"event_index\", x._4)\n",
    "            ) AS alignment,\n",
    "            model_sub\n",
    "        FROM (\n",
    "            SELECT \n",
    "                b.trace_id, \n",
    "                b.node_id, \n",
    "                b.previous_id, \n",
    "                b.model_sub,\n",
    "                calculateAlignmentCost(\n",
    "                    b.model_sub, \n",
    "                    event_array\n",
    "                ) AS calc_alignment  \n",
    "            FROM BASE_DATA b \n",
    "            INNER JOIN SUFFIXES s ON b.trace_id = s.trace_id \n",
    "                AND b.previous_id = s.previous_id \n",
    "                AND b.node_id = s.candidate\n",
    "        )\n",
    "    ),\n",
    "    UPDATED_BASE_DATA AS (\n",
    "        SELECT \n",
    "            b.*,\n",
    "            i.alignment \n",
    "        FROM BASE_DATA b\n",
    "        LEFT JOIN INTERMEDIATE i ON b.trace_id = i.trace_id \n",
    "            AND i.current_id LIKE CONCAT(\"%\",b.model_sub, '%') \n",
    "    ),\n",
    "UPDATED_ALIGNMENTS AS(SELECT len(model_sub)/2,trace_id,time_stamp,previous_events,current_event_level,len,event_array,label,previous_alignment,cost_of_alignment,event_index,node_id,level,TRANSFORM(\n",
    "                    alignment,\n",
    "                        x -> CASE WHEN x.event_index > event_index+1 OR len(model_sub) = 0 and x.move_type in (\"log\",\"sync\") THEN named_struct('event', x.event, 'move_type', \"log\",\"_cost\",1,\"index\",x.event_index)\n",
    "                        WHEN x.move_type in (\"model\",\"sync\") and len(model_sub)/2 < x.event_index THEN named_struct('event',\"\",'move_type',\"\",'_cost',0,\"index\",x.event_index)\n",
    "                        else named_struct('event', x.event, 'move_type', x.move_type,\"_cost\",x._cost,\"index\",x.event_index) END)\n",
    "                         AS new_alignment FROM UPDATED_BASE_DATA)\n",
    "\n",
    "      SELECT  trace_id,\n",
    "                time_stamp as ts,\n",
    "                node_id as current_id,\n",
    "                concat(previous_events,concat_ws(\"\",event_array)) as previous_events,\n",
    "                current_event_level + len as event_level,\n",
    "                level as current_node_level,\n",
    "                label,\n",
    "                TRANSFORM(\n",
    "                        new_alignment,\n",
    "                            x -> named_struct('event', x.event, 'move_type', x.move_type)\n",
    "                        \n",
    "                 ) AS alignment,\n",
    "                cost_of_alignment + aggregate(new_alignment._cost, 0, (acc, x) -> acc + x) AS cost_of_alignment,\n",
    "                event_array,\n",
    "                event_index FROM UPDATED_ALIGNMENTS \n",
    "\"\"\")\n",
    "        result_df = result_df.withColumn(\"batch_id\",F.lit(batch_id))\n",
    "\n",
    "        result_df.write.format(\"delta\").mode(\"append\").option(\"checkpointLocation\", \"/tmp/delta/state_append_30033/\").saveAsTable(\"state_test_batch\")\n",
    "\n",
    "query = streaming_df_batch.writeStream.foreachBatch(process_batch).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7dd11ee-7b5b-4b3f-8d87-3716344c916f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from state_test_batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbfd318f-5898-4050-a24f-ac2d5e65af35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using session_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f04a24-be61-4142-ab48-e56007c4fcc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE iws_event_session (event STRING, time_stamp TIMESTAMP, trace_id STRING);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d473364-3f2a-4a57-a944-5980068c3ce4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "event_df_session = spark.readStream.table(\"iws_event_session\").withWatermark(\"time_stamp\", \"0 seconds\")\n",
    "event_df_session = event_df.withColumn(\"batch_ts\", current_timestamp())\n",
    "event_df_session = event_df_session \\\n",
    "    .withWatermark(\"batch_ts\", \"1 minute\") \\\n",
    "    .groupBy(\"trace_id\", F.session_window(\"batch_ts\", \"5 minutes\")) \\\n",
    "    .agg(F.array_sort(F.collect_list(struct(\"time_stamp\", \"event\"))).alias(\"sorted_events\"))\n",
    "event_df_session.createOrReplaceTempView(\"events_session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fccef3f-1cca-4953-ab55-07382cc15bbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from events_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f7d5cb-ee58-4807-969c-824b3d724ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "event_df_session = spark.readStream.table(\"iws_event_session\").withWatermark(\"time_stamp\", \"0 seconds\")\n",
    "event_df_session.createOrReplaceTempView(\"events_session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eee2679-9633-4d88-89d4-ef7fdbc880c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE state_test_session\n",
    "(trace_id STRING, ts STRUCT<ts1 ARRAY<TIMESTAMP>,cur_ts TIMESTAMP>,time_window STRUCT<start Timestamp,end Timestamp>,current_id STRING,previous_events STRING,event_level INTEGER,current_event_level INTEGER,current_node_level INTEGER,labels ARRAY<STRING>,len INTEGER,node_id STRING,label STRING,alignment ARRAY<STRUCT<event String,move_type String>>,cost_of_alignment INTEGER);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24cbc570-17cd-4873-a04a-8bb1a30cede5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE temp VIEW stream_test_alignm_session AS SELECT DISTINCT \n",
    "                trace_id,\n",
    "                node_id AS current_id,\n",
    "                previous_events,\n",
    "                alignment,\n",
    "                cost_of_alignment,\n",
    "                event_level,\n",
    "                current_event_level,\n",
    "                current_node_level,\n",
    "                rn\n",
    "FROM   (\n",
    "        SELECT *,\n",
    "        Max(current_event_level) OVER (partition BY trace_id) AS max_event_level,\n",
    "        Row_number() OVER (partition BY trace_id,node_id ORDER BY event_level DESC,cost_of_alignment ASC, Len(previous_events) DESC) rn\n",
    "        FROM state_test_session)\n",
    "WHERE rn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dede0c3-1b65-49e2-8e38-69645baf9b5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT trace_id , ts ,time_window,current_id,previous_events ,event_level ,current_event_level ,current_node_level ,labels,len ,node_id ,label ,concat(align,transform(\n",
    "    alignm, \n",
    "    x -> named_struct('event', x.event, 'move_type', x.move_type)\n",
    "  )) as alignment, cost_+array_max(alignm.cost) as cost_of_alignment \n",
    "  FROM(\n",
    "SELECT trace_id,STRUCT(ts as ts1,current_timestamp() as cur_ts) as ts,time_window ,current_id ,cost_of_alignment as cost_ ,concat(previous_events,concat_ws(\"\",labels)) AS previous_events, calculate_alignment(current_id,node_id,labels) as alignm,alignment as align ,current_event_level + len as event_level ,current_event_level + len as current_event_level ,level as current_node_level ,labels ,len ,node_id ,m.label\n",
    "FROM (\n",
    "    SELECT trace_id,\n",
    "           ts,\n",
    "           event_index,\n",
    "           label,\n",
    "           time_window,\n",
    "           current_id,\n",
    "           cost_of_alignment,\n",
    "           previous_events,\n",
    "           alignment,\n",
    "           event_level,\n",
    "           current_event_level,\n",
    "           current_node_level,\n",
    "           labels,\n",
    "           size(labels) as len\n",
    "    FROM (\n",
    "        SELECT e.trace_id,\n",
    "               idx AS event_index, \n",
    "               time_window,\n",
    "               col.label as label, \n",
    "               e.event_arr.label as labels,\n",
    "               e.event_arr.time_stamp as ts,\n",
    "               COALESCE(r.current_id, 'root') AS current_id, \n",
    "               COALESCE(cost_of_alignment, 0) AS cost_of_alignment, \n",
    "               COALESCE(r.previous_events, '') AS previous_events, \n",
    "               COALESCE(r.alignment,array(struct(\"\" as event, \"\" as move_type)))  as alignment,\n",
    "               COALESCE(event_level, 0) AS event_level, \n",
    "               COALESCE(current_event_level, 0) AS current_event_level, \n",
    "               COALESCE(current_node_level, 0) AS current_node_level\n",
    "        FROM (\n",
    "            SELECT trace_id,\n",
    "                   array_sort(collect_list(struct(time_stamp, label))) AS event_arr, session_window(time_stamp, '1 minute') AS time_window\n",
    "            FROM events_session e \n",
    "            JOIN iws_labels l ON e.event = l.event\n",
    "            GROUP BY trace_id, time_window\n",
    "        ) e \n",
    "        LEFT JOIN stream_test_alignm_session r ON e.trace_id = r.trace_id\n",
    "        LATERAL VIEW posexplode(event_arr)  AS idx, col\n",
    "    ) q\n",
    ") f\n",
    "JOIN iws_model m ON (m.node_id LIKE CONCAT(f.current_id, '%')\n",
    "                 AND m.level < f.current_node_level + f.event_index + 3\n",
    "AND m.label = f.label) or m.node_id = f.current_id) -- no match then log moves\n",
    "\"\"\").writeStream.format(\"delta\").outputMode(\"append\").option(\"checkpointLocation\",\"/tmp/delta/state_append_30029/\").toTable(\"state_test_session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52f3e5d-5ef2-4230-b002-63c95fea7083",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from stream_test_alignm_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be131eb4-6d73-4a64-8167-bf1cdf0f39a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from iws_event_session e join iws_labels l on e.event = l.event where trace_id = \"trace_4\" order by time_stamp desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b63ca07e-7ebe-45a0-888e-5f14703cb456",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Add events to iws_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8531d17-e036-4459-bdee-99e8da4965cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"event\", StringType(), True),\n",
    "    StructField(\"time_stamp\", TimestampType(), True),\n",
    "    StructField(\"trace_id\", StringType(), True)\n",
    "])\n",
    "streaming_df_batch_5 = spark.readStream \\\n",
    "    .format(\"parquet\").schema(schema) \\\n",
    "    .load('path_to_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65bf85b7-1955-45cf-a47c-9b9fd37610a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "279ab82b-a328-437e-a047-bf140497850c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE iws_model AS\n",
    "SELECT \"root\" AS node_id, \"-\" AS label, array(\"A\") AS children_labels, array(\"root-A\") AS children_id, \n",
    "    array(\n",
    "        struct(\"root-A\" AS node_id, \"A\" AS label, 1 as level, array() as events_between),\n",
    "        struct(\"root-A-B\" AS node_id, \"B\" AS label, 2 as level, array(\"A\") as events_between),\n",
    "        struct(\"root-A-B-X\" AS node_id, \"X\" AS label, 3 as level, array(\"A\",\"B\") as events_between), \n",
    "        struct(\"root-A-B-D\" AS node_id, \"D\" AS label, 3 as level, array(\"A\",\"B\") as events_between)\n",
    "    ) AS nth_children, 0 as level\n",
    "UNION ALL \n",
    "SELECT \"root-A\", \"A\", array(\"B\"), array(\"root-A-B\"), \n",
    "    array(\n",
    "        struct(\"root-A-B\" AS node_id, \"B\" AS label, 2 as level, array() as events_between),\n",
    "        struct(\"root-A-B-X\" AS node_id, \"X\" AS label, 3 as level, array(\"B\") as events_between),\n",
    "        struct(\"root-A-B-D\" AS node_id, \"D\" AS label, 3 as level, array(\"B\") as events_between),\n",
    "        struct(\"root-A-B-D-C\" AS node_id, \"C\" AS label, 4 as level, array(\"B\",\"D\") as events_between)\n",
    "    ),1\n",
    "UNION ALL \n",
    "SELECT \"root-A-B\", \"B\", array(\"X\", \"D\"), array(\"root-A-B-X\", \"root-A-B-D\"), \n",
    "    array(\n",
    "        struct(\"root-A-B-X\" AS node_id, \"X\" AS label, 3 as level, array() as events_between), -- \"A-B-C\" now \"A-B-X\"\n",
    "        struct(\"root-A-B-D\" AS node_id, \"D\" AS label, 3 as level, array() as events_between),\n",
    "        struct(\"root-A-B-D-C\" AS node_id, \"C\" AS label, 4 as level, array(\"D\") as events_between)\n",
    "    ),2\n",
    "UNION ALL \n",
    "SELECT \"root-A-B-X\", \"X\", array(), array(),\n",
    "    array(),3\n",
    "UNION ALL \n",
    "SELECT \"root-A-B-D\", \"D\", array(\"C\"), array(\"root-A-B-D-C\"), \n",
    "    array(\n",
    "        struct(\"root-A-B-D-C\" AS node_id, \"C\" AS label, 4 as level, array() as events_between),\n",
    "        struct(\"root-A-B-D-C-D\" AS node_id, \"D\" AS label, 5 as level, array(\"C\") as events_between),\n",
    "        struct(\"root-A-B-D-C-C\" AS node_id, \"C\" AS label, 5 as level, array(\"C\") as events_between)\n",
    "    ),3\n",
    "UNION ALL \n",
    "SELECT \"root-A-B-D-C\", \"C\", array(\"D\", \"C\"), array(\"root-A-B-D-C-D\", \"root-A-B-D-C-C\"), \n",
    "    array(\n",
    "        struct(\"root-A-B-D-C-D\" AS node_id, \"D\" AS label, 5 as level, array() as events_between),\n",
    "        struct(\"root-A-B-D-C-C\" AS node_id, \"C\" AS label, 5 as level, array() as events_between)\n",
    "    ),4\n",
    "UNION ALL \n",
    "SELECT \"root-A-B-D-C-D\", \"D\", array(), array(), array(),5\n",
    "UNION ALL \n",
    "SELECT \"root-A-B-D-C-C\", \"C\", array(), array(), array(),5;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011d274f-aad5-49d7-89d7-d42b264cb68e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE iws_event \n",
    "(event STRING,time_stamp TIMESTAMP,trace_id STRING);\n",
    "---SELECT \"A\" event, CURRENT_TIMESTAMP() time_stamp, \"trace_id_0\" trace_id;\n",
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE iws_event_state\n",
    "(event STRING, time_stamp TIMESTAMP, trace_id STRING,processed STRING);\n",
    "\n",
    "CREATE OR REPLACE TABLE iws_labels \n",
    "SELECT \"A\" AS event, \"A\"  AS label\n",
    "UNION ALL\n",
    "SELECT \"B\",\"B\" \n",
    "UNION ALL\n",
    "SELECT \"C\",\"C\" \n",
    "UNION ALL\n",
    "SELECT \"D\",\"D\" \n",
    "UNION ALL\n",
    "SELECT \"X\",\"X\" ;\n",
    "\n",
    "CREATE OR REPLACE TABLE iws_state\n",
    "(trace_id STRING, ts TIMESTAMP, current_node STRING,current_id STRING,cost_of_alignment INTEGER,previous_events STRING, trace STRING, execution_sequence STRING,event_level INTEGER,current_node_level INTEGER);\n",
    "--event level to filter out the latest alignments later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87c6016f-546b-4d49-82de-441a39baf857",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "activities = [\"A\",\"B\",\"C\",\"D\",\"X\"]\n",
    "trace_id_lower = 0\n",
    "trace_id_upper = 9\n",
    "\n",
    "def insert_event(event=None, trace_id=None):\n",
    "    if not event:\n",
    "        event = random.choice(activities)\n",
    "    if not trace_id:\n",
    "        trace_id = 0 #random.randint(trace_id_lower, trace_id_upper)\n",
    "    spark.sql(f\"INSERT INTO iws_event SELECT '{event}', CURRENT_TIMESTAMP(), 'trace_id_{trace_id}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a57845d9-0db2-455f-93ee-c254b52d07c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW latest_state AS \n",
    "SELECT DISTINCT trace_id, ts, current_node, current_id, cost_of_alignment,previous_events,trace,execution_sequence,event_level, max_event_level as current_event_level,current_node_level,rn,write_ts FROM (\n",
    "SELECT *, row_number() OVER (PARTITION BY trace_id,current_id order by event_level desc,cost_of_alignment asc, len(previous_events) desc) rn,\n",
    "max(event_level) OVER (PARTITION BY trace_id) AS max_event_level FROM iws_state\n",
    ") WHERE rn = 1 --and event_level > current_event_level -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c910fd0-9407-4597-ac34-aeb3b49f28d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#if looking at older events, cost should be difference in event levels also number of skips >> between is same number\n",
    "#current level -> used to track what event the state is from if the state is used in the future\n",
    "#event level -> used to track the current event level (how many events there have been)\n",
    "#current_node_level -> used to track what level the node is, used in calculation\n",
    "spark.sql(\"\"\"\n",
    "SELECT trace_id,ts,exploded.*,previous_events FROM(\n",
    "SELECT trace_id,ts,og_event,concat(model_moves,exploded_struct) as sub_exploded,previous_events FROM(\n",
    "SELECT \n",
    "      e.trace_id, \n",
    "      e.time_stamp as ts,\n",
    "      e.og_event,\n",
    "      concat(e.previous_events,e.event) as previous_events,\n",
    "      transform(\n",
    "        filter(m.nth_children, x -> x.label = e.event),\n",
    "        x -> struct(e.event as current_node, x.node_id as current_id, e.cost_of_alignment + abs(x.level - current_node_level-1) + current_event_level - event_level as cost_of_alignment,  concat(coalesce(trace,\"root\"),repeat(\">>\",abs(x.level - current_node_level-1)),e.event) as trace,CONCAT(execution_sequence, coalesce(CONCAT_WS('', x.events_between)),e.event) as execution_sequence, current_event_level+1 as event_level, x.level as current_node_level)\n",
    "      ) as model_moves,\n",
    "      array(struct(e.current_node as current_node, m.node_id as current_id, e.cost_of_alignment + e.current_event_level - e.event_level + 1 as cost_of_alignment, concat(trace, e.event) as trace, concat(execution_sequence, \">>\")as execution_sequence, e.current_event_level + 1 as event_level, current_node_level))\n",
    "      AS exploded_struct\n",
    "    FROM \n",
    "      (SELECT \n",
    "      e.trace_id,\n",
    "      e.time_stamp,\n",
    "      e.event as og_event, \n",
    "      l.label as event, \n",
    "      COALESCE(r.current_node, \"-\") as current_node, \n",
    "      COALESCE(r.current_id, \"root\") as current_id, \n",
    "      coalesce(cost_of_alignment, 0) as cost_of_alignment, \n",
    "      coalesce(r.previous_events,\"\") as previous_events , \n",
    "      coalesce(trace,\"\") as trace, \n",
    "      coalesce(execution_sequence,\"\") as execution_sequence, \n",
    "      coalesce(event_level, 0) as event_level, \n",
    "      coalesce(current_event_level, 0) as current_event_level, \n",
    "      coalesce(current_node_level,0) as current_node_level FROM events e LEFT JOIN latest_state r ON e.trace_id = r.trace_id JOIN iws_labels l on e.event = l.event) e \n",
    "    JOIN iws_model m ON e.current_id = m.node_id ))\n",
    "  LATERAL VIEW explode(sub_exploded) t AS exploded\n",
    "\"\"\").writeStream.format(\"delta\").outputMode(\"append\").option(\"checkpointLocation\",\"/tmp/delta/state_append_69/\").toTable(\"iws_state\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 126681601269680,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Final Demo Notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
